\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{mathtools}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{microtype}
\usepackage{float}
\usepackage{adjustbox}
\usepackage{booktabs,makecell,tabularx}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{verbatim}
\usepackage{xcolor}
\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{epsfig} % for postscript graphics files
\usepackage{mathptmx} % assumes new font selection scheme installed
\usepackage{multicol}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\restylefloat{table}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}
\title{Swarm and Evolutionary Based Algorithms used for Optimization}
\author{
	\IEEEauthorblockN{Augusto Mathias Adams\IEEEauthorrefmark{1}, Caio Phillipe Mizerkowski\IEEEauthorrefmark{2}, Christian Piltz Araújo\IEEEauthorrefmark{3} and Vinicius Eduardo dos Reis\IEEEauthorrefmark{4}}\\
	\IEEEauthorblockA{\IEEEauthorrefmark{1}GRR20172143, augusto.adams@ufpr.br, \IEEEauthorrefmark{2}GRR20166403, caiomizerkowski@gmail.com,\\ \IEEEauthorrefmark{3}GRR20172197, christian0294@yahoo.com.br, \IEEEauthorrefmark{4}GRR20175957, eduardo.reis02@gmail.com}
}

\maketitle


\begin{abstract}
    In this paper, a study of evolution and swarm based algorithms is presented,
    using two classical engineering problems: \textit{Spring Tension} and \textit{Pressure Vessel
    Designs}. The test code for the problems was made using the \textit{Python Language},
    version 3.10 and uses \textit{MealPy} package, version 2.5.1,  to provide the algorithms.
    The algorithms were randomly chosen from a vast list of \textit{MealPy}'s algorithms:
    \textit{Evolutionary Programming (LevyEP)}, \textit{Evolution Strategies (OriginalES)} and
    \textit{Genetic Algorithm (BaseGA)} from \textit{evolutionary\_based} subpackage;
    \textit{Bees Algorithm (OriginalBeesA)}, \textit{Firefly Algorithm (OriginalFFA)} and
    \textit{Particle Swarm Optimization (OriginalPSO)} from \textit{swarm\_based} subpackage.
    Each problem was modeled using \textit{SciPy} package, with constraints implemented as
    \textit{penalty functions}. Each algorithm were optimized separately to extract the best
    solutions from each problem using the \textit{MealPy}'s \textit{Tuner} utility.
    The results, however, are dependant of algorithm and/or problem solved and the Friedman's chi
    squared test for similarity make it noticeable because, although the values for best fits are
    similar, running the same algorithm with different initial conditions does not converge to similar
    values.
\end{abstract}

\begin{IEEEkeywords}
	Optimization Methods, Evolutionary Programming, Evolutionary and Swarm Based Strategies.
\end{IEEEkeywords}

\section{Definitions}
\label{sec:definitions}

The main objective of this paper is study evolutionary and swarm intelligence algorithms.
We present the main concepts of these two algorithm's classes, along with the chosen algorithms
definitions in this section. All citations made in this document are due to Swarm Intelligence classes
and to \textit{MealPy}'s documentation, which points out the theoretical documentation for each
implemented algorithm.

\textit{\textbf{Evolution}}: From Jason Brownlee's \textit{``Clever Algorithms''} -
Evolutionary Algorithms belong to the Evolutionary Computation
field of study concerned with computational methods inspired by the process
and mechanisms of biological evolution. The process of evolution by
means of natural selection (descent with modification) was proposed by
Darwin to account for the variety of life and its suitability (adaptive
fit) for its environment. The mechanisms of evolution describe how
evolution actually takes place through the modification and propagation
of genetic material (proteins). Evolutionary Algorithms are concerned
with investigating computational systems that resemble simplified ver-
sions of the processes and mechanisms of evolution toward achieving
the effects of these processes and mechanisms, namely the development
of adaptive systems. Additional subject areas that fall within the realm
of Evolutionary Computation are algorithms that seek to exploit the
properties from the related fields of Population Genetics, Population
Ecology, Coevolutionary Biology, and Developmental Biology.

\textit{\textbf{Swarm Intelligence}}: From Jason Brownlee's \textit{``Clever Algorithms''} -
Swarm intelligence is the study of computational systems inspired by
the ‘collective intelligence’. Collective Intelligence emerges through the
cooperation of large numbers of homogeneous agents in the environment.
Examples include schools of fish, flocks of birds, and colonies of ants.
Such intelligence is decentralized, self-organizing and distributed through
out an environment. In nature such systems are commonly used to
solve problems such as effective foraging for food, prey evading, or
colony re-location. The information is typically stored throughout the
participating homogeneous agents, or is stored or communicated in
the environment itself such as through the use of pheromones in ants,
dancing in bees, and proximity in fish and birds.

\textit{\textbf{Evolutionary Programming:}} From Jason Brownlee's \textit{``Clever Algorithms''} -
Evolutionary Programming is a Global Optimization algorithm and is an instance of an Evolutionary
Algorithm from the field of Evolutionary Computation. The approach is a sibling of other Evolutionary
Algorithms such as the Genetic Algorithm, and Learning Classifier Systems. It is sometimes confused with Genetic
Programming given the similarity in name, and more recently it shows a strong functional similarity to Evolution Strategies.
Evolutionary Programming is inspired by the theory of evolution by means of natural selection. Specifically, the technique is inspired by
macro-level or the species-level process of evolution (phenotype, hereditary, variation) and is not concerned with the genetic mechanisms of
evolution (genome, chromosomes, genes, alleles).

\textit{\textbf{Evolutionary Strategies:}} From Jason Brownlee's \textit{``Clever Algorithms''} -
Evolution Strategies is a global optimization algorithm and is an instance of an Evolutionary Algorithm from the field of Evolutionary
Computation. Evolution Strategies is a sibling technique to other Evolutionary Algorithms such as Genetic Algorithms (Section 3.2), Genetic
Programming (Section 3.3), Learning Classifier Systems, and Evolutionary Programming. A popular descendant of
the Evolution Strategies algorithm is the Covariance Matrix Adaptation Evolution Strategies (CMA-ES).

\textit{\textbf{Genetic Algorithms:}} From Jason Brownlee's \textit{``Clever Algorithms''} -
The Genetic Algorithm is an Adaptive Strategy and a Global Optimization technique. It is an Evolutionary Algorithm and belongs to the
broader study of Evolutionary Computation. The Genetic Algorithm is a sibling of other Evolutionary Algorithms such as Genetic Programming,
 Evolution Strategies, Evolutionary Programming, and Learning Classifier Systems. The Genetic Algorithm is a parent of a large number of variant techniques
and sub-fields too numerous to list. The Genetic Algorithm is inspired by population genetics (including heredity and gene frequencies),
and evolution at the population level, as well as the Mendelian understanding of the structure (such as chromosomes, genes, alleles) and mechanisms
(such as recombination and mutation). This is the so-called new or modern synthesis of evolutionary biology.

\textit{\textbf{Particle Swarm Optimization:}} From Jason Brownlee's \textit{``Clever Algorithms''} - Particle Swarm Optimization belongs to the field of
Swarm Intelligence and Collective Intelligence and is a sub-field of Computational Intelligence. Particle Swarm Optimization is related to other Swarm Intelligence
algorithms such as Ant Colony Optimization and it is a baseline algorithm for many variations, too numerous to list. It is inspired by the social foraging behavior
of some animals such as flocking behavior of birds and the schooling behavior of fish.


\textit{\textbf{Firefly Algorithm:}} From Xin-She Yang \textit{``Nature-Inspired Metaheuristic Algorithms''} -
The flashing light of fireflies is an amazing sight in the summer sky in the tropical and temperate regions. There are about two thousand firefly
species, and most fireflies produce short and rhythmic flashes. The pattern of flashes is often unique for a particular species. The flashing light is
produced by a process of bioluminescence, and the true functions of suchsignaling systems are still being debated. However, two fundamental functions
of such flashes are to attract mating partners (communication), and to attract potential prey. In addition, flashing may also serve as a protective warning
mechanism to remind potential predators of the bitter taste of fireflies. The firefly algorithm tries to mimic the attractiveness of Fireflies and has three
basic rules:

\begin{itemize}
    \item All fireflies are unisex so that one firefly will be attracted to other
          fireflies regardless of their sex;
    \item Attractiveness is proportional to the their brightness, thus for any two
          flashing fireflies, the less brighter one will move towards the brighter
          one. The attractiveness is proportional to the brightness and they
          both decrease as their distance increases. If there is no brighter one
          than a particular firefly, it will move randomly;
    \item The brightness of a firefly is affected or determined by the landscape
          of the objective function.
\end{itemize}


\textit{\textbf{Bees Algorithm:}} From Jason Brownlee's \textit{``Clever Algorithms''} -
The Bees Algorithm beings to Bee Inspired Algorithms and the field of Swarm Intelligence, and more broadly the fields of Computational
Intelligence and Metaheuristics. The Bees Algorithm is related to other Bee Inspired Algorithms, such as Bee Colony Optimization, and other
Swarm Intelligence algorithms such as Ant Colony Optimization and Particle Swarm Optimization.
It is inspired by the foraging behavior of honey bees. Honey bees collect nectar from vast areas around their hive (more than
10 kilometers). Bee Colonies have been observed to send bees to collect nectar from flower patches relative to the amount of food available at
each patch. Bees communicate with each other at the hive via a waggle dance that informs other bees in the hive as to the direction, distance,
and quality rating of food sources.

\section{Methodology}
\label{sec:methodology}

\subsection{Optimization Problem Selection}
\label{subsec:optimization-problem-selection}
The two problems selected for this paper were \textit{Spring Tension Design} and \textit{Pressure Vessel Design}. Although it was simple
to choose the first two problems from the computational work statements, the choice was more than justified because these aroblems are well
known in the literature.
Thus, the problem selection was driven by which has more than one source to compare results.

\subsubsection{Spring Tension Design}
\label{subsubsec:methodology-spring-tension-design}
From \textit{Nature-Inspired Metaheuristic Algorithms - Xin-She Yang} -
The design of a tension and compression spring is a well-known benchmark
optimization problem.
The main aim is to minimize the weight subject
to constraints on deflection, stress, surge frequency and geometry. It involves
three design variables: the wire diameter $x_1$ , coil diameter $x_2$ and
number/length of the coil $x_3$.
This problem can be summarized as:

\begin{equation}
    f (x) = x_1^2 x_2 (2 + x_3)
\end{equation}

subject to

\begin{equation}
    \begin{split}
        g_1(x) = \frac{x_2^3 x_3}{71785 x_1^4} \leq 0\\
        g_2(x) = \frac{4 x_2^2 - x_1 x_2}{12566 \left( x_1^3 x_2 - x_1^4\right)}\\
        + \frac{1}{5108 x_1^2} -1 \leq 0\\
        g_3(x) = 1 - \frac{140.45 x_1}{x_2^2 x_3} \leq 0\\
        g_4 = \frac{x_1 + x_2}{1.5} - 1 \leq 0
    \end{split}
\end{equation}

The original bounding limits of the variables, extracted from computational work
statements, are:

\begin{equation}
    \begin{split}
        0.05 \leq x_1 \leq 2.0\\
        0.25 \leq x_2 \leq 1.3\\
        2.0 \leq x_3 \leq 15.0
    \end{split}
\end{equation}

\subsubsection{Pressure Vessel Design}
\label{subsubsec:methodology-pressure-vessel-design}
From \textit{Solving Design of Pressure Vessel Engineering
Problem Using a Fruit Fly Optimization Algorithm - XIANTING KE et al} -
A pressure vessel design model involves four decision variables: $x_1$ is defined
thickness of the pressure vessel $T_s$ , $x_2$ stands for
thickness of the head $T_H$ , $x_3$ represents inner radius of the
vessel $R$ , and $x_4$ is on behalf of length of the vessel
barring head $L$ , the total variables described as $( x_1 , x_2 , x_3 , x_4 )$.
The objective function of the problem is to minimize the total cost, including
the cost of material, forming, and welding.
The general pressure vessel design optimization model is expressed as:

\begin{equation}
	\begin{split}
        f(x) = 0.6224 x_1 x_3 x_4 + 1.7781 x_2 x_3^2 \\
        + 3.1661 x_1^2 x_4 + 19.84 x_1^2 x_3
	\end{split}
\end{equation}

subject to:

\begin{equation}
    \begin{split}
        g_1(x) = - x_1 + 0.0193 x_3 \leq 0\\
        g_2(x) = - x_2 + 0.00954 x_3 \leq 0\\
        g_3(x) = - \pi x_3^2 x_4 -\frac{4}{3}\pi x_3^3 + 1296000 \leq 0\\
        g_4(x) = x_4 -240 \leq 0\\
    \end{split}
\end{equation}

The original bounding limits of the variables, extracted from computational work
statements, are:

\begin{equation}
    \begin{split}
        0 \leq x_1 \leq 99\\
        0 \leq x_2 \leq 99\\
        10 \leq x_3 \leq 200\\
        10 \leq x_4 \leq 200\\
    \end{split}
\end{equation}

For some reason these settings does not work at all with the selected optimizers, so
some research in the literature \textit{Solving Design of Pressure Vessel Engineering
Problem Using a Fruit Fly Optimization Algorithm - XIANTING KE et al} suggest the
following bounding limits:

\begin{equation}
    \begin{split}
        0.0625 \leq x_1 \leq 99 \times 0.0625\\
        0.0625 \leq x_2 \leq 99 \times 0.0625\\
        10 \leq x_3 \leq 200\\
        10 \leq x_4 \leq 200
    \end{split}
\end{equation}

But these settings produce many random, bizarre and noisy results in all selected optimizers. Then
a proud-and-lame-homemade set of variable boundings comes in handy, obtained by tweaking the original
boundings:

\begin{equation}
    \begin{split}
        0.75 \leq x_1 \leq 0.8\\
        0.35 \leq x_2 \leq 0.4\\
        39.5 \leq x_3 \leq 41.0\\
        195.0 \leq x_4 \leq 205.0\\
    \end{split}
\end{equation}

It is not intended here to point out modeling errors of any kind, nor point out package
errors made by the authors or ours, but the homemade bounding limits was necessary to
reach the literature results.

\subsection{Constraint Implementation}
\label{subsec:methodology-constraint-implementation}
The constraints that all problems are subjected were implemented as \textit{penalty functions},
that is, it adds a high value when the constraint is not satisfied, zero otherwise. It is an optional requirement
from computational work in question and is the recommended way to put constraints from \textit{MealPy}'s
Manual.

\subsection{Programming Language}
\label{subsec:methodology-programming-language}

The chosen programming language for the test code was \textit{Python Language}, version 3.10, because it is an opensource language easy to
program and has a huge amount of packages regarding artificial intelligence, genetic algorithms and swarm based algorithms.
From these packages it was selected \textit{MealPy} package, version 2.5.1, because it comprises all the algorithm's classed tested in this paper.

\subsection{Algorithm Selection}
\label{subsec:methodology-algorithm-selection}

The algorithm selection was made in two steps: first, it was extracted simple version of the two classes (evolutionary and swarm based) and
then it was used a simple shuffle using Python's \textit{random.shuffle} in a terminal - no script was required.

The chosen algorithms were:

\begin{itemize}
    \item \textit{\textbf{Evolution Based Algorithms: }} LevyEP (Evolutionary Programming),
    OriginalES (Evolution Strategy) and BaseGA (Genetic Algorithm)
    \item \textit{\textbf{Swarm Based Algorithms: }} OriginalBeesA (Bees Algorithm),
    OriginalFFA (Original Firefly Algorithm) and OriginalPSO (Particle Swarm Optimization)
\end{itemize}

\subsection{Optimizer Tuning}
\label{subsec:methodology-optimizer-tuning}

The hyperparameters for each optimizer were tuned using the \textit{MealPy}'s \textit{Tuner} utility.
It is a recomended procedure to tune hyperparameters for each optimizer and problem, according to
\textit{MealPy}'s Manual.
The \textit{Tuner} utility is a very simple grid search metaheuristic search tool
that test each grid configuration for a specified oprimizer runs.
Although simple, it is a very expensive procedure that took 2 days to complete. It was defined 10 runs for each
configuration, with the following set of hyperparameter:

\begin{itemize}
    \item \textit{\textbf{Evolution Based Algorithms: }}
        \subitem \textit{LevyEP:}
            \subsubitem \textit{bout\_size (float)}: percentage of child agents implement tournament selection.
        \subitem \textit{OriginalES:}
            \subsubitem \textit{lamda (float)}: Percentage of child agents evolving in the next generation.
        \subitem \textit{BaseGA:}
            \subsubitem \textit{pc (float)}: cross-over probability
            \subsubitem \textit{pm (float)}: mutation probability
    \item \textit{\textbf{Swarm Based Algorithms: }}
        \subitem \textit{OriginalBeesA:}
            \subsubitem \textit{selected\_site\_ratio (float)}
            \subsubitem \textit{elite\_site\_ratio (float)}
            \subsubitem \textit{selected\_site\_bee\_ratio (float)}
            \subsubitem \textit{elite\_site\_bee\_ratio (float)}
            \subsubitem \textit{dance\_radius (float)}
            \subsubitem \textit{dance\_reduction (float)}
        \subitem \textit{OriginalFFA:}
            \subsubitem \textit{gamma (float)}: Light Absorption Coefficient
            \subsubitem \textit{beta\_base (float)}: Attraction Coefficient Base Value
            \subsubitem \textit{alpha (float)}: Mutation Coefficient
            \subsubitem \textit{alpha\_damp (float)}: Mutation Coefficient Damp Rate
            \subsubitem \textit{delta (float)}: Mutation Step Size
            \subsubitem \textit{exponent (int)}: Exponent
        \subitem \textit{OriginalPSO:}
            \subsubitem \textit{c1 (float)}: local coefficient
            \subsubitem \textit{c2 (float)}: global coefficient
            \subsubitem \textit{w\_min (float)}: Weight min of bird
            \subsubitem \textit{w\_max (float)}: Weight max of bird

\end{itemize}

\subsection{Optimizer Parameters}
\label{subsec:methodology-optimizer-parameters}

For all optimizers and problems, were selected the following parameters:

\begin{itemize}
    \item \textit{Runs}: 100 runs
    \item \textit{Epochs}: 100 epochs
    \item \textit{Population}: 100 initial points
\end{itemize}

The initial solutions were randomly selected using the same seed for all algorithms.

\section{Results}
\label{sec:results}
\subsection{Spring Tension Design}
\label{ssubec:spring_problem}

\begin{table}[H]
\centering
\caption{Statistical Information about function values for Spring Tension Design}
\label{function_values:spring_problem}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lrrrrr}
\toprule
Algorithm &      Min F &     Mean F &   Median F &      Max F &   StdDev F \\
\midrule
       EP & 0.01250534 & 0.01255960 & 0.01253156 & 0.01453933 & 0.00020190 \\
       ES & 0.01252956 & 0.01281857 & 0.01274283 & 0.01346214 & 0.00024181 \\
       GA & 0.01252305 & 0.01628575 & 0.01616768 & 0.02561808 & 0.00273276 \\
    BeesA & 0.01250200 & 0.01268010 & 0.01261691 & 0.01406679 & 0.00021629 \\
      FFA & 0.01249803 & 0.01254780 & 0.01253499 & 0.01268116 & 0.00003152 \\
      PSO & 0.01252626 & 0.01390866 & 0.01318774 & 0.03036537 & 0.00342240 \\
\bottomrule
\end{tabular}}
\end{table}


\begin{figure}[H]
\centering
\caption{Boxplot for Spring Tension Design}
\label{fig:spring_tension_design_boxplot}
\includegraphics[scale=0.5]{images/spring_problem_boxplot.png}
\end{figure}


\begin{figure}[H]
\centering
\caption{Convergence lines for Spring Tension Design}
\label{fig:spring_problem_convergence}
\includegraphics[width= 0.4 \textwidth]{images/spring_problem_convergence.png}
\end{figure}
    
\subsection{Pressure Vessel Design (Original)}
\label{subsec:pressure_vessel_problem_original}

\begin{table}[H]
\centering
\caption{Statistical Information about function values for Pressure Vessel Design (Original)}
\label{function_values:pressure_vessel_problem_original}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lrrrrr}
\toprule
Algorithm &         Min F &         Mean F &       Median F &          Max F &       StdDev F \\
\midrule
       EP &    0.08150806 &     0.15642850 &     0.09093798 &     1.32637552 &     0.16807714 \\
       ES &    0.08161190 &   718.92910990 &     1.00278859 &  9443.54269260 &  2060.07774212 \\
       GA &   17.06920868 &   525.00734162 &   361.53879602 &  3112.03503082 &   454.52386703 \\
    BeesA & 2393.70106213 & 17564.33024022 & 15751.91467117 & 53850.38271384 & 10514.60780411 \\
      FFA &    0.08306113 &     0.13296127 &     0.13106271 &     0.20084847 &     0.02396876 \\
      PSO &    0.08149204 &     0.24191732 &     0.08159173 &    10.00611523 &     1.00072349 \\
\bottomrule
\end{tabular}}
\end{table}

\begin{figure}[H]
\centering
\caption{Boxplot for Pressure Vessel Design (Original)}
\label{fig:pressure_vessel_design_original_boxplot}
\includegraphics[scale=0.5]{images/pressure_vessel_problem_original_boxplot.png}
\end{figure}


\begin{figure}[H]
\centering
\caption{Convergence lines for Pressure Vessel Design (Original)}
\label{fig:pressure_vessel_problem_original_convergence}
\includegraphics[width= 0.4 \textwidth]{images/pressure_vessel_problem_original_convergence.png}
\end{figure}

    
\subsection{Pressure Vessel Design}
\label{subsec:pressure_vessel_problem}

\begin{table}
\centering
\caption{Statistical Information about function values for Pressure Vessel Design}
\label{function_values:pressure_vessel_problem}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lrrrrr}
\toprule
Algorithm &         Min F &        Mean F &      Median F &         Max F &   StdDev F \\
\midrule
       EP & 5534.57345496 & 5534.63426098 & 5534.61416291 & 5534.92993607 & 0.05804548 \\
       ES & 5534.57917966 & 5535.28165367 & 5535.06149254 & 5538.03003895 & 0.65098467 \\
       GA & 5534.83662424 & 5538.36788889 & 5536.92932748 & 5550.91639342 & 3.57079951 \\
    BeesA & 5534.57516657 & 5537.97495231 & 5537.04824684 & 5548.55587234 & 3.28239740 \\
      FFA & 5534.57401617 & 5534.58849395 & 5534.58435493 & 5534.63296149 & 0.01341777 \\
      PSO & 5534.57345480 & 5534.57381593 & 5534.57355270 & 5534.57625290 & 0.00053848 \\
\bottomrule
\end{tabular}
}
\end{table}

\begin{figure}[H]
\centering
\caption{Boxplot for Pressure Vessel Design}
\label{fig:pressure_vessel_design_boxplot}
\includegraphics[scale=0.5]{images/pressure_vessel_problem_boxplot.png}
\end{figure}


\begin{figure}[H]
\centering
\caption{Convergence lines for Pressure Vessel Design}
\label{fig:pressure_vessel_problem_convergence}
\includegraphics[width=0.4 \textwidth]{images/pressure_vessel_problem_convergence.png}
\end{figure}

\subsection{Significance Test}
\label{subsec:pressure_vessel_problem_significance_test}

\begin{table}[H]
\centering
\caption{Significance Test Using Friedman Chi-Squared Test}
\label{friedman_test}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lrr}
\toprule
                          Problem &   Statistics &    p-value \\
\midrule
            Spring Tension Design & 339.05714286 & 0.00000000 \\
           Pressure Vessel Design & 436.16000000 & 0.00000000 \\
Pressure Vessel Design (Original) & 425.07428571 & 0.00000000 \\
\bottomrule
\end{tabular}
}
\end{table}



\section{Discussion}

The main objective of this work is demonstrate the inner work of a optimization class of algorithms called \textit{Quasi-Newton algorithms}. Four of them are presented, along with the results and analysis regarding convergence, function evaluations and function objective values.

For implementation, it was decided to put constraints on search space, because for the original search space of some functions, like Ackley and Rastrigin, there are other minimas that will confuse every algorithm tested for this paper, and there is no sense on test a local minima procedure on these conditions. To restrain the initial solution to some region in the search space was a cheap and ready solution to avoid the algorithms to stop because of stumbling upon the wrong minimum.


The \textit{Levenberg-Marquardt Algorithm} implemented for this paper is very sensitive to hyperparameters, namely $\alpha$ and $\lambda$, which was not tunned to every function, so, the conspicuous performance variation is noticeable along the results. LMA is known to be fast and reliable, but in our implementation it was found a little tricky to get the hyperparameters right for all the test functions. But for Rosenbrock and Rastrigin, it is the best option, even for lame implementations like ours.

The \textit{Broyden–Fletcher–Goldfarb–Shanno} Algorithm was the only one that we decided to use it as is from \textit{SciPy}, because it has features desired for this work, like counting of function evaluations, gradient evaluation and iterations. It performed very well for Ackley and Booth Functions, and reasonably well for other functions.

The \textit{Davidon–Fletcher–Powell} Algorithm is not implemented in \textit{SciPy} but its equations shows that it is the dual of BFGS, so, it was easy to implement using some \textit{SciPy} code for BFGS algorithm. In the DFP procedure, there is a Hessian inversion to be evaluated every iteration, then, numerical imprecision would be taken into consideration evaluating its results. While in some cases it has a superb performance (Like Ackley Function), it has very poor performance in Rosenbrock Function and even in Beale Function. The increase of dimensions makes DFP suffer of numerical imprecision due to the Hessian inversion, except for Rastrigin, and it is the loser algorithm in other functions.

Finally, the \textit{Limited memory BFGS} has an average performance among the functions. The original algorithm was implemented using code found in the url https://github.com/qkolj/L-BFGS/blob/master/L-BFGS.ipynb.

In fact, there are no Quasi-Newton algorithms that solver the minima problem for an any function. From the standpoint of performance and convergence, all the algorithms have an average performance and a reasonable convergence, although we have to restrain the search space to a small area near the optimal solution.

\end{document}
